{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85bb5bf",
   "metadata": {},
   "source": [
    "# MAST Plasma Volume\n",
    "\n",
    "Predict plasma volume from frames captured by a wide-angle visible spectrum camera on the CCFE's Mega Ampere Spherical Tokamak (MAST).\n",
    "\n",
    "> **Note:** Run the `Plasma Volume Dataset.ipynb` notebook first to download the required dataset from the FAIR-MAST server. This ensures you have the necessary data files in the `fair_mast_data/plasma_volume` directory.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook addresses the second of three Data Science challenges for the ITER International School 2024.\n",
    "\n",
    "The animation below shows footage from a wide-angle proton bullet camera installed on MAST. This visible spectrum camera captures high frame-rate recordings showing the complete plasma cross-section on both sides of the central column. Similar cameras first captured visual recordings of ELM structures on MAST and AUG in 2007.\n",
    "\n",
    "![MAST Proton Camera Animation](../media/images/c3_proton_camera.gif)\n",
    "\n",
    "**Challenge Goal:** Develop a machine learning algorithm that predicts plasma volume from a single frame of the camera feed.\n",
    "\n",
    "This challenge introduces techniques for inferring parameters from 2D image data.\n",
    "\n",
    "The open-source MAST Data Catalog provides all data for this challenge. Credit to Samuel Jackson, Nathan Cummings, Saiful Khan, and the wider MAST community for creating this FAIR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6742e0",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The image below shows the maximum plasma volume achieved for all shots in the MAST M9 campaign. These experiments were the last performed before the major upgrade to create the current MAST-U machine.\n",
    "\n",
    "![Maximum Plasma Volume](../media/images/plasma_volume.png)\n",
    "\n",
    "Maximum plasma volumes range from ~6 to ~10 cubic meters.\n",
    "\n",
    "This challenge requires you to predict plasma volume from camera imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b68d5",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "The `./fair_mast_data/plasma_volume` directory contains all necessary files for this challenge.\n",
    "\n",
    "### Available Files\n",
    "- `train.nc` - Training dataset in netCDF format\n",
    "- `test.nc` - Test dataset in netCDF format\n",
    "\n",
    "### Data Structure\n",
    "- `shot_id` - Unique identifier for each shot\n",
    "- `frame` - Stack of camera frames with dimensions (shot_id, height, width)\n",
    "- `plasma_volume` - Target plasma volume in cubic meters (m³)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c2836",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Both training and test datasets use the netCDF format. This self-describing format includes important metadata such as image dimensions alongside the data itself.\n",
    "\n",
    "After importing the prerequisite libraries, load the datasets using the xarray library as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import pathlib\n",
    "from importlib import resources\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.decomposition\n",
    "import sklearn.ensemble\n",
    "import sklearn.kernel_ridge\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import xarray as xr\n",
    "\n",
    "pkg_path = resources.files(\"data_science_challenges\")\n",
    "data_path = pkg_path / \"fair_mast_data\" / \"plasma_volume\"\n",
    "\n",
    "try:\n",
    "    with (\n",
    "        xr.open_dataset(data_path / \"train.nc\") as train,\n",
    "        xr.open_dataset(data_path / \"test.nc\") as test,\n",
    "    ):\n",
    "        train = train.load()\n",
    "        test = test.load()\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\n",
    "        \"Please run the `Plasma Volume Dataset.ipynb` notebook to download the \"\n",
    "        \"required dataset from the FAIR-MAST server\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257f556",
   "metadata": {},
   "source": [
    "**Important:** The camera images have dimensions (shot_id, height, width). You need to reshape this data to the (n_samples, n_features) format required by scikit-learn. After reshaping, you can split the data using standard train-test split techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbebff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.frame.values.reshape(train.sizes[\"shot_id\"], -1)\n",
    "y = train.volume\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17cda5",
   "metadata": {},
   "source": [
    "### Model Pipeline\n",
    "\n",
    "The camera frames have high dimensionality. To make the problem computationally tractable, we apply dimensionality reduction techniques.\n",
    "\n",
    "The pipeline below uses KernelPCA decomposition as a preprocessing step. This component provides several hyperparameters you can tune to improve model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9126615",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.decomposition.KernelPCA(n_components=25),\n",
    "    sklearn.linear_model.LinearRegression(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b52a4",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "With the preprocessing pipeline set up, fit the model to your training data and evaluate its performance on the test set.\n",
    "\n",
    "#### Understanding Model Evaluation Metrics\n",
    "\n",
    "For this challenge, we use the R² score (Coefficient of Determination) to evaluate model performance. This metric indicates the proportion of variance in the dependent variable (plasma volume) that can be predicted from the independent variables (camera frame data).\n",
    "\n",
    "- **R² = 1.0**: Perfect predictions (model explains 100% of the variance)\n",
    "- **R² = 0.0**: Model performs no better than always predicting the mean value\n",
    "- **R² < 0.0**: Model performs worse than predicting the mean value\n",
    "\n",
    "For this plasma volume prediction task, R² values closer to 1.0 indicate better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_predict = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate R² score (coefficient of determination)\n",
    "# Higher values indicate better model fit (1.0 is perfect, 0.0 is equivalent to predicting the mean)\n",
    "R2 = sklearn.metrics.r2_score(y_test, y_predict)\n",
    "print(f\"Model R²: {R2:.3f} (higher is better)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c95dc",
   "metadata": {},
   "source": [
    "### Making Predictions and Generating Submissions\n",
    "\n",
    "After validating our model performance, we can generate predictions for the test dataset and prepare a submission file. Remember to reshape the test frames to match the (n_samples, n_features) format that your model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0473853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the test dataset\n",
    "volume = pipeline.predict(test.frame.values.reshape(test.sizes[\"shot_id\"], -1))\n",
    "\n",
    "# Create and save submission file\n",
    "solution = pd.DataFrame(\n",
    "    {\"plasma_volume\": volume}, index=pd.Index(test.shot_id, name=\"shot_id\")\n",
    ")\n",
    "solution.to_csv(path / \"linear_regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's improve our model with hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'kernelpca__n_components': [25, 50, 100],\n",
    "    'kernelpca__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'linearregression__fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_predict_best = best_model.predict(X_test)\n",
    "R2_best = sklearn.metrics.r2_score(y_test, y_predict_best)\n",
    "print(f\"Best model R2: {R2_best:.3f}\")\n",
    "\n",
    "# Generate final predictions\n",
    "best_volume = best_model.predict(test.frame.values.reshape(test.sizes[\"shot_id\"], -1))\n",
    "best_solution = pd.DataFrame(\n",
    "    {\"plasma_volume\": best_volume}, index=pd.Index(test.shot_id, name=\"shot_id\")\n",
    ")\n",
    "best_solution.to_csv(path / \"optimized_regression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f593f4",
   "metadata": {},
   "source": [
    "### Results Visualization\n",
    "\n",
    "The plot below compares predicted plasma volume values against ground truth measurements. This visualization helps assess how well our model performs across the range of plasma volumes.\n",
    "\n",
    "- **Perfect predictions** would fall along the dashed gray line (y = x)\n",
    "- **Points above the line** represent overestimations (predicted > actual)\n",
    "- **Points below the line** represent underestimations (predicted < actual)\n",
    "\n",
    "#### Understanding the R² Score\n",
    "\n",
    "The R² score (coefficient of determination) provides a statistical measure of how well the model's predictions approximate the actual data points:\n",
    "\n",
    "- **R² = 1.0**: The model perfectly explains all variation in the data\n",
    "- **R² = 0.8**: The model explains 80% of the variance in the plasma volume\n",
    "- **R² = 0.5**: Only half of the variance is explained by the model\n",
    "- **R² ≤ 0**: The model provides no useful information for predicting plasma volume\n",
    "\n",
    "Unlike error metrics (where lower is better), higher R² values indicate superior model performance. The R² score is also scale-independent, making it useful for comparing model performance across different datasets and target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f57cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a DataFrame with actual and predicted values for easy plotting\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_predict_best\n",
    "})\n",
    "\n",
    "sns.scatterplot(    \n",
    "    x='Actual', \n",
    "    y='Predicted', \n",
    "    data=results_df,\n",
    "    alpha=0.6,\n",
    "    )\n",
    "\n",
    "# Add perfect prediction line (y=x)\n",
    "plt.plot(\n",
    "    [results_df['Actual'].min(), results_df['Actual'].max()], \n",
    "    [results_df['Actual'].min(), results_df['Actual'].max()], \n",
    "    '--', \n",
    "    color='gray'\n",
    ")\n",
    "\n",
    "# Annotate with R2 score\n",
    "plt.annotate(\n",
    "    f'R² = {R2_best:.3f}', \n",
    "    xy=(0.05, 0.95), \n",
    "    xycoords='axes fraction',\n",
    "    fontsize=14, \n",
    "    bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8)\n",
    ")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Actual Plasma Volume (m³)', fontsize=12)\n",
    "plt.ylabel('Predicted Plasma Volume (m³)', fontsize=12)\n",
    "plt.title('Actual vs. Predicted Plasma Volume', fontsize=14)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf249c7b",
   "metadata": {},
   "source": [
    "### Model Assessment and Next Steps\n",
    "\n",
    "Looking at the visualization above, we can observe several potential outliers in our dataset that may be affecting model performance. Let's assess the impact of these outliers and discuss strategies for improvement.\n",
    "\n",
    "#### Influence of Outliers\n",
    "\n",
    "The scatter plot reveals data points that deviate significantly from the general trend. These outliers could represent:\n",
    "\n",
    "1. **Experimental anomalies**: Unusual plasma conditions during specific shots\n",
    "2. **Measurement errors**: Issues with camera calibration or frame acquisition\n",
    "3. **Real but rare phenomena**: Valid but uncommon plasma behaviors\n",
    "\n",
    "Outliers can disproportionately influence model training, especially with algorithms sensitive to extreme values. Linear regression, in particular, can have its coefficients skewed by these points, reducing overall prediction accuracy for the majority of cases.\n",
    "\n",
    "#### Outlier Handling Strategies\n",
    "\n",
    "To address the outlier issue, consider these preprocessing approaches:\n",
    "\n",
    "1. **Statistical filtering**: Remove data points beyond 3 standard deviations from the mean\n",
    "2. **Interquartile range (IQR) method**: Filter points outside 1.5 × IQR\n",
    "3. **DBSCAN clustering**: Use density-based clustering to identify and exclude anomalies\n",
    "4. **Winsorization**: Cap extreme values instead of removing them entirely\n",
    "\n",
    "Implementing these techniques would likely improve model robustness and overall R² score by allowing the model to focus on capturing the primary relationship patterns rather than accommodating extreme cases.\n",
    "\n",
    "#### Additional Recommendations\n",
    "\n",
    "Beyond outlier handling, consider these improvements to enhance model performance:\n",
    "\n",
    "1. **Feature engineering**: \n",
    "   - Extract higher-level features from raw image data using edge detection or contour analysis\n",
    "   - Apply dimensionality reduction techniques more selective than PCA (e.g., t-SNE, UMAP)\n",
    "   - Create domain-specific features based on tokamak physics\n",
    "\n",
    "2. **Advanced models**:\n",
    "   - Convolutional Neural Networks (CNNs) to directly process image data\n",
    "   - Ensemble methods like Random Forests or Gradient Boosting for better generalization\n",
    "   - Support Vector Regression with non-linear kernels for complex relationships\n",
    "\n",
    "3. **Cross-validation strategies**:\n",
    "   - Group k-fold cross-validation based on shot_id to prevent data leakage\n",
    "   - Time-series aware validation if temporal relationships exist\n",
    "\n",
    "4. **Optimization techniques**:\n",
    "   - Expand hyperparameter search space\n",
    "   - Use Bayesian optimization instead of grid search for efficiency\n",
    "   - Apply regularization to reduce overfitting\n",
    "\n",
    "5. **Data augmentation**:\n",
    "   - Generate synthetic data through minor image transformations\n",
    "   - Simulate camera variability to improve model robustness\n",
    "\n",
    "With these enhancements, especially the outlier handling techniques, we could expect a significant improvement in model performance, potentially increasing the R² score by 5-15% depending on the extent of outlier influence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_challenges",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
