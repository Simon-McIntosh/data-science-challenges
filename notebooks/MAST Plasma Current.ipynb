{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "334db3c0",
   "metadata": {},
   "source": [
    "# MAST Plasma Current\n",
    "\n",
    "Infer plasma current from CCFE's Mega Ampere Spherical Tokamak using discrete magnetic diagnostic data.\n",
    "\n",
    "> **Note:** The dataset files for this challenge should be available in the `fair_mast_data/plasma_current` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf23e3b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook addresses the first of three Data Science challenges for the ITER International School 2024.\n",
    "\n",
    "**Challenge Goal:** Reconstruct plasma current time-series waveforms from the Mega Ampere Spherical Tokamak (MAST) using magnetic field measurements.\n",
    "\n",
    "The open-source MAST Data Catalog provides all data for this challenge. Credit to Samuel Jackson, Nathan Cummings, Saiful Khan, and the MAST community for creating this FAIR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a9275b",
   "metadata": {},
   "source": [
    "## Description\n",
    "Tokamaks measure plasma current with high precision. In this challenge, you must reconstruct plasma current waveforms by applying machine learning to magnetic diagnostic signals.\n",
    "\n",
    "We've removed information about the location, orientation, and calibration of diagnostic measurements to prevent using traditional inference techniques.\n",
    "\n",
    "This challenge presents a labeled regression problem. Your model must transform one-dimensional signals into accurate target waveforms.\n",
    "\n",
    "The image below shows signals and targets from the first shot in the training dataset: \n",
    "\n",
    "![MAST Plasma Signals and Targets](../media/images/plasma_current.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ebd48",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "The `./fair_mast_data/plasma_current` directory contains data files for the MAST Plasma Current challenge.\n",
    "\n",
    "### Available Files\n",
    "Both train and test files use a simple CSV format with a single header.\n",
    "\n",
    "- `train.csv` - Training dataset\n",
    "- `test.csv` - Test dataset\n",
    "\n",
    "### Data Structure\n",
    "- `index` - Unique identifier for each sample\n",
    "- `time` - Measurement time in seconds\n",
    "- `XMA_CCBV04:XMA_CCBV36` - Magnetic measurements from the center_column group in volts\n",
    "- `XMA_OBR03:XMA_OBR17.1` - Magnetic measurements from the outer_discrete group in volts\n",
    "- `plasma_current` - Target plasma current in kiloamperes (kA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836036c",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The code below demonstrates how to prepare a competition submission. This basic approach provides a starting point that you'll need to improve to achieve competitive scores.\n",
    "\n",
    "First, we'll import the necessary libraries for data processing, visualization, and machine learning. Python libraries are organized at the top in three groups: standard library, third-party packages, and local modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b801ca",
   "metadata": {},
   "source": [
    "## Installation Instructions\n",
    "\n",
    "### Local Installation\n",
    "\n",
    "1. Clone the repository\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/Simon-McIntosh/data-science-challenges.git\n",
    "   ```\n",
    "\n",
    "2. Navigate to the project directory\n",
    "\n",
    "   ```bash\n",
    "   cd data-science-challenges\n",
    "   ```\n",
    "\n",
    "3. Install [uv](https://docs.astral.sh/uv/getting-started/installation/#standalone-installer) if you don't have it already\n",
    "\n",
    "   ```bash\n",
    "   pip install uv\n",
    "   ```\n",
    "\n",
    "4. Create a virtual environment and install dependencies using uv\n",
    "\n",
    "   ```bash\n",
    "   uv venv\n",
    "   uv pip install -e .\n",
    "   ```\n",
    "\n",
    "5. Activate the virtual environment\n",
    "\n",
    "   ```bash\n",
    "   # On Windows\n",
    "   .venv\\Scripts\\activate\n",
    "\n",
    "   # On Unix or MacOS\n",
    "   source .venv/bin/activate\n",
    "   ```\n",
    "\n",
    "### Google Colab Setup\n",
    "\n",
    "If you're using Google Colab to run this notebook, you'll need to install the project package first. The recommended installation method uses the `uv` package installer for faster dependency resolution:\n",
    "\n",
    "```bash\n",
    "!pip install uv\n",
    "!uv pip install git+https://github.com/Simon-McIntosh/data-science-challenges.git\n",
    "```\n",
    "\n",
    "You may also install directly with pip, though installation might be slower:\n",
    "\n",
    "```bash\n",
    "!pip install git+https://github.com/Simon-McIntosh/data-science-challenges.git\n",
    "```\n",
    "\n",
    "> **Note:** After installation, you may need to restart your runtime for all changes to take effect. You can do this by clicking on the \"Runtime\" menu and selecting \"Restart runtime\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669153ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run when installing in a Colab notebook\n",
    "# !uv pip install git+https://github.com/Simon-McIntosh/data-science-challenges.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bc0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from importlib import resources\n",
    "import pathlib\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c824c",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "First, we locate the data files. The `train.csv` and `test.csv` files are stored in the `fair_mast_data/plasma_current` directory. We use the `pathlib` library to find these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_path = resources.files(\"data_science_challenges\")\n",
    "data_path = pkg_path / \"fair_mast_data\" / \"plasma_current\"\n",
    "\n",
    "print(list(pathlib.Path(data_path).glob(\"*.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97345968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into train and test DataFrames using the stored data_path\n",
    "train = pd.read_csv(data_path / 'train.csv')\n",
    "test = pd.read_csv(data_path / 'test.csv')\n",
    "\n",
    "# Print the first few rows of the train DataFrame\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccca95e",
   "metadata": {},
   "source": [
    "### Model Pipeline\n",
    "Create a scikit-learn pipeline to process training and test data consistently. This example uses a minimal pipeline setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4851374",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.linear_model.LinearRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c7cf8",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "We divide the training data into features (X) and target (y), then split into training and validation sets. This example uses all signal columns as features. A thorough understanding of your data before this step significantly improves results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.drop(\"plasma_current\", axis=1), train.plasma_current\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    "    random_state=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c918f",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "First, we fit the model to the training set and generate predictions for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1d28e",
   "metadata": {},
   "source": [
    "#### Understanding Model Evaluation Metrics\n",
    "\n",
    "We evaluate model performance using multiple metrics. While the challenge uses only Mean Absolute Percentage Error (MAPE) for scoring, additional metrics provide valuable insights during development.\n",
    "\n",
    "**Mean Absolute Percentage Error (MAPE)**\n",
    "\n",
    "MAPE measures the average percentage difference between predicted and actual values. Mathematically, it's calculated as:\n",
    "\n",
    "$MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|$\n",
    "\n",
    "Where:\n",
    "- $y_i$ represents the true plasma current value\n",
    "- $\\hat{y}_i$ represents the predicted plasma current value\n",
    "- $n$ is the number of predictions being evaluated\n",
    "\n",
    "**Why use MAPE for this challenge?**\n",
    "- **Scale-independence**: MAPE expresses error as a percentage, making it useful for comparing performance across different datasets or plasma current magnitudes\n",
    "- **Interpretability**: The percentage format is intuitive for understanding prediction accuracy\n",
    "- **Competition metric**: This is the primary scoring metric for the challenge\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "MAE measures the average magnitude of errors between predicted and actual values, without considering their direction. \n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "\n",
    "**Benefits of MAE:**\n",
    "- **Same units**: Expressed in kiloamperes (kA), making it directly interpretable in the physical context\n",
    "- **Robustness**: Less sensitive to outliers than squared error metrics\n",
    "- **Direct interpretation**: Tells us the average absolute deviation in kA between predictions and actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0baf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "# Lower values indicate better model performance (0% is perfect)\n",
    "mape = sklearn.metrics.mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"MAPE {mape:1.3f} (lower is better)\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) in kiloamperes\n",
    "# This is more intuitive as it's in the same units as plasma current\n",
    "mae = sklearn.metrics.mean_absolute_error(y_test, y_pred)\n",
    "print(f\"MAE {mae:1.3f} kA (lower is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9459f",
   "metadata": {},
   "source": [
    "### Making Predictions and Generating Submissions\n",
    "\n",
    "After validating our model performance, we can generate predictions for the test dataset and prepare a submission file. The Plasma Current challenge requires a CSV file with \"index\" and \"plasma_current\" headers followed by predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f11ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the test dataset\n",
    "predictions = pipe.predict(test)\n",
    "\n",
    "# Create and save submission file\n",
    "submission = pd.DataFrame(predictions, columns=[\"plasma_current\"])\n",
    "submission.index.name = \"index\"\n",
    "\n",
    "# Save the submission file using the stored data_path\n",
    "submission.to_csv(data_path / \"linear_regression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc8fd5",
   "metadata": {},
   "source": [
    "### Results Visualization\n",
    "\n",
    "Visualizing predictions against validation data helps evaluate performance. Since the competition includes multiple plasma pulses, displaying individual waveforms improves interpretation. We first reorder the dataset that was shuffled during splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1102f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\")\n",
    "axes = plt.subplots(figsize=(8, 6))[1]\n",
    "\n",
    "sort_index = np.argsort(X_test.time)\n",
    "_X_test = X_test.iloc[sort_index]\n",
    "_y_test = y_test.iloc[sort_index]\n",
    "for shot_index in np.unique(X_train.shot_index):\n",
    "    index = _X_test.shot_index == shot_index\n",
    "    axes.plot(_X_test.loc[index, \"time\"], _y_test.loc[index], \"--\", color=\"gray\")\n",
    "    axes.plot(_X_test.loc[index, \"time\"], pipe.predict(_X_test)[index])\n",
    "axes.set_xlabel(\"time s\")\n",
    "axes.set_ylabel(\"plasma current kA\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b52be1",
   "metadata": {},
   "source": [
    "### Model Assessment and Next Steps\n",
    "\n",
    "The plot compares actual plasma current (gray dashed lines) with model predictions (colored lines). When evaluating your model, consider these key questions:\n",
    "\n",
    "1. Do the predictions match your expectations?\n",
    "2. How accurately does the model fit the data?\n",
    "3. Is there data leakage between training and validation sets?\n",
    "4. What methods could fix potential leakage issues?\n",
    "\n",
    "For improving model performance, consider:\n",
    "- Using more sophisticated models beyond linear regression\n",
    "- Implementing feature engineering to enhance signal data\n",
    "- Applying appropriate preprocessing techniques\n",
    "- Tuning model hyperparameters\n",
    "\n",
    "Your final ranking depends on your score on the private leaderboard section, which uses a hidden evaluation dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_challenges",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
